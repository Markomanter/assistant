# stt.py

import numpy as np
import sounddevice as sd
from faster_whisper import WhisperModel

import config

print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –º–æ–¥–µ–ª—å faster-whisper ({config.WHISPER_MODEL_NAME})‚Ä¶")

whisper_model = WhisperModel(
    config.WHISPER_MODEL_NAME,
    device=config.WHISPER_DEVICE,              # –∑ config
    compute_type=config.WHISPER_COMPUTE_TYPE,  # "int8" –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
)


def _rms(x: np.ndarray) -> float:
    """–°–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è (–≥—Ä—É–±–æ ‚Äî –≥—É—á–Ω—ñ—Å—Ç—å —Ñ—Ä–µ–π–º—É)."""
    if x.size == 0:
        return 0.0
    return float(np.sqrt(np.mean(np.square(x), dtype=np.float64)))


def record_audio() -> np.ndarray:
    """
    –°–ª—É—Ö–∞—î–º–æ –º—ñ–∫—Ä–æ—Ñ–æ–Ω, –ø–æ–∫–∏:
    - –Ω–µ –∑ º—è–≤–∏—Ç—å—Å—è –≥–æ–ª–æ—Å (–≥—É—á–Ω—ñ—Å—Ç—å > VAD_THRESHOLD),
    - –∞ –ø–æ—Ç—ñ–º –Ω–µ –±—É–¥–µ —Ç–∏—à—ñ VAD_SILENCE_SECONDS –ø—ñ–¥—Ä—è–¥.

    –¢–∞–∫ –∞—Å–∏—Å—Ç–µ–Ω—Ç —Å–∞–º —Ä–æ–∑—É–º—ñ—î, –∫–æ–ª–∏ —Ç–∏ –∑–∞–∫—ñ–Ω—á–∏–≤ –≥–æ–≤–æ—Ä–∏—Ç–∏.
    """
    sr = config.SAMPLE_RATE
    FRAME_DURATION = 0.2  # —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —Ñ—Ä–µ–π–º—É –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    frame_samples = int(sr * FRAME_DURATION)

    vad_threshold = getattr(config, "VAD_THRESHOLD", 0.01)
    vad_silence_seconds = getattr(config, "VAD_SILENCE_SECONDS", 0.8)
    max_record_seconds = getattr(config, "MAX_RECORD_SECONDS", 20)

    print("üéô –°–ª—É—Ö–∞—é –º—ñ–∫—Ä–æ—Ñ–æ–Ω... –ì–æ–≤–æ—Ä–∏, —ñ —è –∑—É–ø–∏–Ω—é—Å—è, –∫–æ–ª–∏ –±—É–¥–µ –ø–∞—É–∑–∞.")

    audio_chunks: list[np.ndarray] = []
    started = False
    silence_time = 0.0
    total_time = 0.0

    with sd.InputStream(samplerate=sr, channels=1, dtype="float32") as stream:
        while True:
            frame, _ = stream.read(frame_samples)  # shape: (frame_samples, 1)
            frame = frame.reshape(-1)
            total_time += FRAME_DURATION

            level = _rms(frame)

            if not started:
                # –ß–µ–∫–∞—î–º–æ, –ø–æ–∫–∏ –∑ º—è–≤–∏—Ç—å—Å—è –≥–æ–ª–æ—Å
                if level > vad_threshold:
                    started = True
                    print("üéô –í–∏—è–≤–∏–≤ –≥–æ–ª–æ—Å, –∑–∞–ø–∏—Å—É—é...")
                    audio_chunks.append(frame.copy())
                else:
                    # —â–µ —Ç–∏—à–∞ –¥–æ –ø–æ—á–∞—Ç–∫—É ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ
                    continue
            else:
                # –≤–∂–µ –∑–∞–ø–∏—Å—É—î–º–æ
                audio_chunks.append(frame.copy())

                if level < vad_threshold:
                    silence_time += FRAME_DURATION
                    if silence_time >= vad_silence_seconds:
                        print("‚èπ –í–∏—è–≤–ª–µ–Ω–æ –ø–∞—É–∑—É, –∑—É–ø–∏–Ω—è—é –∑–∞–ø–∏—Å.")
                        break
                else:
                    # –∑–Ω–æ–≤—É –≥–æ–ª–æ—Å ‚Äî –æ–±–Ω—É–ª—è—î–º–æ —Ç–∞–π–º–µ—Ä —Ç–∏—à—ñ
                    silence_time = 0.0

                if total_time >= max_record_seconds:
                    print("‚èπ –î–æ—Å—è–≥–Ω—É—Ç–æ MAX_RECORD_SECONDS, –∑—É–ø–∏–Ω—è—é –∑–∞–ø–∏—Å.")
                    break

    if not audio_chunks:
        print("‚ö† –ù–µ –æ—Ç—Ä–∏–º–∞–≤ –∂–æ–¥–Ω–æ–≥–æ –∑–≤—É–∫—É.")
        return np.zeros(0, dtype="float32")

    audio = np.concatenate(audio_chunks).astype("float32")
    return audio


def transcribe_audio(audio: np.ndarray):
    """
    –ü—Ä–∏–π–º–∞—î numpy-–º–∞—Å–∏–≤ float32 [N] 16kHz —ñ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î –≤ faster-whisper.
    –ü–æ–≤–µ—Ä—Ç–∞—î (text, lang_code).
    """
    if audio is None or audio.size == 0:
        return "", "unknown"

    # –ù–∞ –≤—Å—è–∫–∏–π –≤–∏–ø–∞–¥–æ–∫ –∑–≤–µ–¥–µ–º–æ –¥–æ 1D
    if audio.ndim > 1:
        audio = audio[:, 0]

    if config.SAMPLE_RATE != 16000:
        # –£ —Ç–µ–±–µ SAMPLE_RATE = 16000, —Ç–æ–º—É —Ü–µ–π –±–ª–æ–∫ –º–æ–∂–Ω–∞ —ñ–≥–Ω–æ—Ä—É–≤–∞—Ç–∏.
        # –Ø–∫—â–æ –∫–æ–ª–∏-–Ω–µ–±—É–¥—å –ø–æ–º—ñ–Ω—è—î—à ‚Äî —Ç—É—Ç —Ç—Ä–µ–±–∞ –±—É–¥–µ –¥–æ–¥–∞—Ç–∏ —Ä–µ—Å–µ–º–ø–ª—ñ–Ω–≥.
        pass

    print("üß† –†–æ–∑–ø—ñ–∑–Ω–∞—é —Ç–µ–∫—Å—Ç...")

    segments, info = whisper_model.transcribe(
        audio,
        beam_size=3,
        language=None,  # –∞–≤—Ç–æ-–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
    )

    text_chunks = [seg.text for seg in segments]
    text = " ".join(text_chunks).strip()
    lang = (info.language or "unknown").lower()

    print(f"üìù –†–æ–∑–ø—ñ–∑–Ω–∞–Ω–∏–π —Ç–µ–∫—Å—Ç: {text!r}")
    print(f"üåê –í–∏–∑–Ω–∞—á–µ–Ω–∞ –º–æ–≤–∞: {lang}")
    return text, lang
